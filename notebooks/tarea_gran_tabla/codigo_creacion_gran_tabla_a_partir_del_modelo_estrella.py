# -*- coding: utf-8 -*-
"""Codigo Creacion Gran Tabla a partir del Modelo Estrella.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pPSvvKQ-XNm-m7RFhgihzSFPfT5SMcn6
"""

import pandas as pd
import re
from pathlib import Path
import re, unicodedata

# ========= 1) CONFIG =========
folder = Path(r"/content/A123")   # <-- CAMBIA ESTA RUTA

# Nombres que identifican "hechos" por nombre de archivo
pat_fact = re.compile(r"(hecho|hechos|fact)", re.I)

def read_any(path: Path) -> dict:
    """Devuelve {nombre_tabla: DataFrame}. Para Excel retorna una entrada por hoja."""
    tables = {}
    if path.suffix.lower() == ".csv":
        df = pd.read_csv(path, dtype="object", encoding="utf-8", on_bad_lines="skip")
        tables[path.stem] = df
    elif path.suffix.lower() in [".xlsx", ".xls"]:
        xls = pd.ExcelFile(path)
        for sh in xls.sheet_names:
            df = pd.read_excel(xls, sheet_name=sh, dtype="object")
            name = f"{path.stem}__{sh}"           # ‚Üê nombre compuesto Archivo__Hoja
            tables[name] = df
    else:
        raise ValueError(f"Formato no soportado: {path.name}")
    return tables

# Cargar TODO
tables = {}
for f in folder.iterdir():
    if f.suffix.lower() in [".csv",".xlsx",".xls"]:
        try:
            tables.update(read_any(f))
        except Exception as e:
            print(f"[ADVERTENCIA] No pude leer {f.name}: {e}")

# (opcional) ver qu√© tablas se cargaron
print("Tablas cargadas:", list(tables.keys()))

# patr√≥n para detectar hechos (si no tienes, se usar√° la de m√°s filas)
pat_fact = re.compile(r"(hecho|hechos|fact)", re.I)

def guess_fact_name(tables_dict):
    candidates = [n for n in tables_dict if pat_fact.search(n)]
    return max(candidates, key=lambda n: len(tables_dict[n])) if candidates else \
           max(tables_dict, key=lambda n: len(tables_dict[n]))

fact_name = guess_fact_name(tables)
fact = tables[fact_name].copy()
dim_names = [n for n in tables if n != fact_name]

# üîß Forzar llaves (dimensi√≥n -> lista de columnas con el MISMO nombre en hechos y dimensi√≥n)
claves_forzadas = {
    # AJUSTA el nombre EXACTO de la tabla (recuerda que puede ser Archivo__Hoja)
    "MiArchivo__Dimension_municipos_de_naciminetos_R": ["COD_DANE_NAC_PR"]
}

# Opcional: fuerza llaves por dimensi√≥n (cuando lo sepas)
# claves_forzadas = {"Dim_Cliente": ["id_cliente"], "Dim_Producto": ["id_producto"]}

print(f"[INFO] Hechos detectada: {fact_name} ({len(fact)} filas, {fact.shape[1]} columnas)")
print(f"[INFO] Dimensiones detectadas: {dim_names}")

# ========= 4) HEUR√çSTICAS DE LLAVES Y MERGE =========
def normalize_cols(df):
    return [re.sub(r"\s+", "_", str(c).strip().lower()) for c in df.columns]

def candidate_keys_for_dim(dim_df, fact_df):
    dim_norm = normalize_cols(dim_df)
    fact_norm = normalize_cols(fact_df)
    common_norm = set(dim_norm).intersection(fact_norm)

    # mapas norm->original
    dim_map = {re.sub(r"\s+", "_", str(c).strip().lower()): c for c in dim_df.columns}
    common_dim_cols = [dim_map[c] for c in common_norm]

    # 1) ¬øalguna columna com√∫n es √∫nica en la dimensi√≥n?
    unique_cols = [c for c in common_dim_cols
                   if dim_df[c].nunique(dropna=True) == len(dim_df)]
    if unique_cols:
        return unique_cols[:1]  # usa la m√°s clara

    # 2) ¬øconjunto de columnas comunes forma llave compuesta?
    if len(common_dim_cols) >= 2:
        if dim_df[common_dim_cols].drop_duplicates().shape[0] == len(dim_df):
            return common_dim_cols

    # 3) fallback: columnas que parezcan ID
    id_like = [c for c in common_dim_cols if re.search(r"(id_|_id$|^id$)", c, re.I)]
    if id_like:
        return id_like[:1]

    return []  # sin pistas

def safe_left_join(base_df, dim_df, keys, dim_label):
    before_rows = len(base_df)
    # renombrar columnas de la dimensi√≥n (excepto llaves) para evitar colisi√≥n
    non_key_cols = [c for c in dim_df.columns if c not in keys]
    renamed = dim_df.rename(columns={c: f"{dim_label}__{c}" for c in non_key_cols})

    merged = base_df.merge(renamed, on=keys, how="left")
    after_rows = len(merged)
    increased = after_rows > before_rows
    if increased:
        # revertimos si detectamos 1:N
        return base_df, True, []
    added_cols = [c for c in merged.columns if c not in base_df.columns]
    return merged, False, added_cols

gran_tabla = fact.copy()
join_report = []

for dn in dim_names:
    dim_df = tables[dn].copy()

    # ¬øllaves forzadas por configuraci√≥n?
    if dn in claves_forzadas:
        keys = claves_forzadas[dn]
    else:
        keys = candidate_keys_for_dim(dim_df, gran_tabla)

    used = False
    added_cols = []
    increased = None
    note = ""
    if keys:
        try:
            gran_tabla_new, increased, added_cols = safe_left_join(gran_tabla, dim_df, keys, dn)
            if increased:
                note = "Omitida: la uni√≥n aumentaba filas (1:N). Revisa llaves o agrega/agrega por llave."
            else:
                gran_tabla = gran_tabla_new
                used = True
        except Exception as e:
            note = f"Error en merge: {e}"
    else:
        note = "Omitida: sin llaves comunes plausibles."

    join_report.append({
        "dimension": dn,
        "keys_detected": ", ".join(keys) if keys else "",
        "cols_added": len(added_cols),
        "row_count_increased": increased if increased is not None else "",
        "used_in_merge": used,
        "note": note
    })

!pip install xlsxwriter

# ========= 5) EXPORTAR =========
out = folder  # puedes cambiar a otra carpeta de salida
pd.DataFrame(join_report).to_csv(out / "reporte_uniones.csv", index=False, encoding="utf-8")
gran_tabla.to_csv(out / "gran_tabla.csv", index=False, encoding="utf-8")
with pd.ExcelWriter(out / "gran_tabla.xlsx", engine="openpyxl") as writer:
    gran_tabla.to_excel(writer, index=False, sheet_name="gran_tabla")
    pd.DataFrame(join_report).to_excel(writer, index=False, sheet_name="reporte_uniones")

print(f"[OK] Exportado gran_tabla.* y reporte_uniones.csv en: {out}")

display(gran_tabla.iloc[:, :15])

# --- Elimina columnas espec√≠ficas del DataFrame final (case-insensitive) ---

cols_to_remove = [
    "ID_GENERO",
    "ID_AREA_CON_PR",
    "ID_CLAS_PR",
    "Universidad",
    "ID_PERSONA_PR",
    "ID_CONVOCATORIA",
    "COD_DANE_NAC_PR",
    "ID_NIV_FORMACION_PR",
    "COD_DANE_RES_PR",
]

# Mapa lower() -> nombre real de la columna en gran_tabla
lower_map = {c.lower(): c for c in gran_tabla.columns}

# Existentes (case-insensitive) y faltantes
existing = [lower_map[c.lower()] for c in cols_to_remove if c.lower() in lower_map]
missing  = [c for c in cols_to_remove if c.lower() not in lower_map]

print(f"[INFO] Se eliminar√°n {len(existing)} columnas:", existing)
if missing:
    print(f"[INFO] No se encontraron (ok): {missing}")

# Opci√≥n A: crear copia sin IDs
gran_tabla_sin_id = gran_tabla.drop(columns=existing, errors="ignore")



from pathlib import Path
out = folder
gran_tabla_sin_id.to_csv(out / "gran_tabla_sin_id.csv", index=False, encoding="utf-8")
with pd.ExcelWriter(out / "gran_tabla_sin_id.xlsx", engine="openpyxl") as w:
     gran_tabla_sin_id.to_excel(w, index=False)