# -*- coding: utf-8 -*-
"""analisis2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vaRhCkqttWz_fgaGwc6B3ER6lLbHh_3p
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
import numpy as np

excel_file = "/content/GRAN_TABLA(SIN ID).xlsx"
df = pd.read_excel(excel_file)

print("Columna:")
print(df.dtypes)

print("dimensiones de la base:")
print(df.shape)

df['ANO_CONVO'] = pd.to_datetime(df['ANO_CONVO'], errors='coerce')

print("\nLos valores únicos en la columna 'NME_CONVOCATORIA' son:")
print(df['NME_CONVOCATORIA'].unique())

# Contar los valores faltantes por columna
missing_values = df.isnull().sum()
# Filtrar columnas con valores faltantes
columns_with_na = missing_values[missing_values > 0]

print("Columnas con valores faltantes:")
print(columns_with_na)

# Seleccionar solo las columnas numéricas
df_numeric = df.select_dtypes(include=['float64', 'int64'])

# Calcular la matriz de correlación
correlation_matrix = df_numeric.corr()

# Crear el mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Mapa de calor de correlación')
plt.show()

plt.figure(figsize=(12, 7))
ax = sns.countplot(data=df, x='NME_CONVOCATORIA', hue='NME_GENERO_PR', palette='viridis')

plt.title('Distribución de Género por Convocatoria')
plt.xlabel('Convocatoria')
plt.ylabel('Número de Participantes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Calcular totales por convocatoria
totales = df['NME_CONVOCATORIA'].value_counts().to_dict()

# Agregar porcentajes en cada barra
for p in ax.patches:
    # Altura de la barra (conteo)
    conteo = p.get_height()
    # Convocatoria correspondiente
    convocatoria = p.get_x() + p.get_width() / 2.0
    # Identificar la categoría en el eje x
    categoria = ax.get_xticks()[round((p.get_x() + p.get_width()/2) / (ax.get_xticks()[1]-ax.get_xticks()[0]))]

    # Obtener nombre real de la convocatoria
    convocatoria_label = ax.get_xticklabels()[int(p.get_x() + p.get_width()/2)].get_text()

    # Calcular porcentaje
    porcentaje = conteo / totales[convocatoria_label] * 100

    ax.annotate(f'{porcentaje:.1f}%',
                (p.get_x() + p.get_width() / 2., conteo),
                ha='center', va='bottom', fontsize=9, color='black')

# Create a contingency table (cross-tabulation) of the two columns
contingency_table = pd.crosstab(df['NME_CLASIFICACION_PR'], df['NME_REGION_RES_PR'])

# Get the unique combinations of NME_CLASIFICACION_PR and ORDEN_CLAS_PR to use for sorting
classification_order = df[['NME_CLASIFICACION_PR', 'ORDEN_CLAS_PR']].drop_duplicates()

# Set 'NME_CLASIFICACION_PR' as the index for joining
classification_order = classification_order.set_index('NME_CLASIFICACION_PR')

# Join the contingency table with the classification order
contingency_table_ordered = contingency_table.join(classification_order)

# Sort the contingency table by 'ORDEN_CLAS_PR'
contingency_table_ordered = contingency_table_ordered.sort_values(by='ORDEN_CLAS_PR')

# Drop the 'ORDEN_CLAS_PR' column as it's no longer needed in the final table
contingency_table_ordered = contingency_table_ordered.drop(columns='ORDEN_CLAS_PR')

# Display the ordered contingency table
display(contingency_table_ordered)

import pandas as pd
from scipy.stats import chi2_contingency
import numpy as np


# Calculate the chi-squared statistic, p-value, degrees of freedom, and expected frequencies
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Calculate Pearson's contingency coefficient
n = contingency_table.sum().sum() # Total number of observations
contingency_coefficient = np.sqrt(chi2 / (chi2 + n))

print(f"Estadístico Chi-cuadrado: {chi2:.4f}")
print(f"Valor p: {p:.4f}")
print(f"Grados de libertad: {dof}")
print(f"Coeficiente de Contingencia de Pearson: {contingency_coefficient:.4f}")

# Create a contingency table (cross-tabulation) of the two columns
contingency_table_form_region = pd.crosstab(df['NME_NIV_FORM_PR'], df['NME_REGION_RES_PR'])

# Calculate the chi-squared statistic, p-value, degrees of freedom, and expected frequencies
chi2_form_region, p_form_region, dof_form_region, expected_form_region = chi2_contingency(contingency_table_form_region)

# Calculate Pearson's contingency coefficient
n_form_region = contingency_table_form_region.sum().sum() # Total number of observations
contingency_coefficient_form_region = np.sqrt(chi2_form_region / (chi2_form_region + n_form_region))

print(f"Estadístico Chi-cuadrado (Nivel de Formación vs Región de Residencia): {chi2_form_region:.4f}")
print(f"Valor p (Nivel de Formación vs Región de Residencia): {p_form_region:.4f}")
print(f"Grados de libertad (Nivel de Formación vs Región de Residencia): {dof_form_region}")
print(f"Coeficiente de Contingencia de Pearson (Nivel de Formación vs Región de Residencia): {contingency_coefficient_form_region:.4f}")


# Create the heatmap
plt.figure(figsize=(15, 10)) # Adjust figure size as needed
sns.heatmap(contingency_table_form_region, annot=False, cmap='YlGnBu') # annot=False to avoid overcrowding
plt.title('Mapa de Calor de Frecuencia entre NME_NIV_FORM_PR y NME_REGION_RES_PR')
plt.xlabel('NME_REGION_RES_PR')
plt.ylabel('NME_NIV_FORM_PR')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

plt.figure(figsize=(14, 8))
sns.countplot(data=df, x='NME_GRAN_AREA_PR', hue='NME_CLASIFICACION_PR', palette='viridis')

plt.title('Distribución de Clasificación por Gran Área')
plt.xlabel('Gran Área')
plt.ylabel('Número de Participantes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(16, 8)) # Adjusted figure size for potentially more categories
sns.countplot(data=df, x='NME_NIV_FORM_PR', hue='NME_CLASIFICACION_PR', palette='viridis')

plt.title('Distribución de Clasificación por Nivel de Formación')
plt.xlabel('Nivel de Formación')
plt.ylabel('Número de Participantes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

contingency_table = pd.crosstab(df['NME_NIV_FORM_PR'], df['NME_CLASIFICACION_PR'])
# percentages
contingency_table_percentages_row = pd.crosstab(df['NME_NIV_FORM_PR'], df['NME_CLASIFICACION_PR'], normalize='index') * 100

print("\nTabla de Contingencia con Porcentajes (por fila):")
display(contingency_table_percentages_row.round(2)) # Round to 2 decimal places

# Calculate counts for each combination of 'NME_GRAN_AREA_PR' and 'NME_GENERO_PR'
counts = df.groupby(['NME_GRAN_AREA_PR', 'NME_GENERO_PR']).size().reset_index(name='Count')

# Calculate the total count for each 'NME_GRAN_AREA_PR'
totals = counts.groupby('NME_GRAN_AREA_PR')['Count'].sum().reset_index(name='Total')

# Merge counts with totals to calculate percentages
counts = pd.merge(counts, totals, on='NME_GRAN_AREA_PR')
counts['Percentage'] = (counts['Count'] / counts['Total']) * 100

# Create a pivot table for the stacked bar chart
pivot_counts = counts.pivot(index='NME_GRAN_AREA_PR', columns='NME_GENERO_PR', values='Count').fillna(0)

# Create the stacked bar chart
ax = pivot_counts.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='viridis')

plt.title('Distribución de Género por Gran Área (Porcentajes Apilados)')
plt.xlabel('Gran Área')
plt.ylabel('Número de Participantes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Add percentages to the stacked bars
for c in ax.containers:
    labels = [f'{w:.1f}%' if (w := v.get_height()) > 0 else '' for v in c]
    ax.bar_label(c, labels=labels, label_type='center', fontsize=8, color='white') # Added color for visibility

plt.show()

plt.figure(figsize=(14, 8)) # Adjust figure size as needed
sns.countplot(data=df, x='NME_CONVOCATORIA', hue='NME_GRAN_AREA_PR', palette='viridis')

plt.title('Distribución de Gran Área por Convocatoria')
plt.xlabel('Convocatoria')
plt.ylabel('Número de Participantes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

from scipy.stats import chi2_contingency

# Create a contingency table
contingency_table_conv_area = pd.crosstab(df['NME_CONVOCATORIA'], df['NME_GRAN_AREA_PR'])

# Perform the chi-squared test
chi2_conv_area, p_conv_area, dof_conv_area, expected_conv_area = chi2_contingency(contingency_table_conv_area)

print("Resultados de la Prueba Chi-cuadrado para NME_CONVOCATORIA vs NME_GRAN_AREA_PR:")
print(f"Estadístico Chi-cuadrado: {chi2_conv_area:.4f}")
print(f"Valor p: {p_conv_area:.4f}")
print(f"Grados de libertad: {dof_conv_area}")

# Optionally, calculate Pearson's contingency coefficient as a measure of association strength
n_conv_area = contingency_table_conv_area.sum().sum()
contingency_coefficient_conv_area = np.sqrt(chi2_conv_area / (chi2_conv_area + n_conv_area))
print(f"Coeficiente de Contingencia de Pearson: {contingency_coefficient_conv_area:.4f}")



import pandas as pd
from scipy.stats import kruskal
import numpy as np

# Drop rows with missing values in the relevant columns for the test
df_cleaned = df.dropna(subset=['NME_GRAN_AREA_PR', 'EDAD_ANOS_PR']).copy()

# Group the numerical data by the categories of the categorical variable
# Ensure there's more than one group with data for the test
groups_area = [group['EDAD_ANOS_PR'].values for name, group in df_cleaned.groupby('NME_GRAN_AREA_PR') if len(group) > 0]

# Perform the Kruskal-Wallis test only if there are at least two groups with data
if len(groups_area) >= 2:
    statistic_area, p_value_area = kruskal(*groups_area)

    print("Resultados de la Prueba de Kruskal-Wallis para NME_GRAN_AREA_PR vs EDAD_ANOS_PR:")
    print(f"Estadístico Kruskal-Wallis: {statistic_area:.4f}")
    print(f"Valor p: {p_value_area:.4f}")

    # Interpret the result
    alpha = 0.05
    if p_value_area < alpha:
        print(f"El valor p ({p_value_area:.4f}) es menor que el nivel de significancia ({alpha}), por lo tanto, rechazamos la hipótesis nula.")
        print("Existe una diferencia estadísticamente significativa en la distribución de 'EDAD_ANOS_PR' entre las diferentes grandes áreas.")
    else:
        print(f"El valor p ({p_value_area:.4f}) es mayor que el nivel de significancia ({alpha}), por lo tanto, no rechazamos la hipótesis nula.")
        print("No existe una diferencia estadísticamente significativa en la distribución de 'EDAD_ANOS_PR' entre las diferentes grandes áreas.")
else:
    print("No hay suficientes grupos con datos para realizar la prueba de Kruskal-Wallis para NME_GRAN_AREA_PR vs EDAD_ANOS_PR.")

# Install the scikit-posthocs library if you haven't already
!pip install scikit-posthocs

import scikit_posthocs as sp
import pandas as pd

# Drop rows with missing values in the relevant columns
df_cleaned = df.dropna(subset=['NME_GRAN_AREA_PR', 'EDAD_ANOS_PR']).copy()

# Perform Dunn's test with Bonferroni correction
# The input to sp.posthoc_dunn is a list of arrays, where each array contains the data for one group.
# We need to group the data by 'NME_GRAN_AREA_PR' and extract the 'EDAD_ANOS_PR' values for each group.
groups_data = df_cleaned.groupby('NME_GRAN_AREA_PR')['EDAD_ANOS_PR'].apply(list)

# sp.posthoc_dunn requires a list of arrays, so we need to extract the lists from the Series
group_arrays = groups_data.tolist()

# Get the group names for the index and columns of the results table
group_names = groups_data.index.tolist()

# Perform Dunn's test
dunn_results = sp.posthoc_dunn(group_arrays, p_adjust='bonferroni')

# Rename the index and columns with the group names
dunn_results.columns = group_names
dunn_results.index = group_names

print("Resultados de la prueba post-hoc de Dunn (con corrección de Bonferroni) para NME_GRAN_AREA_PR y EDAD_ANOS_PR:")
display(dunn_results)
